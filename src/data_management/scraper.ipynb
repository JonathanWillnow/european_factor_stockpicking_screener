{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json , time, os, difflib, itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing.dummy import Pool\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import random\n",
    "from pandas import ExcelWriter\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy import stats #The SciPy stats module\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(url, index_exchange):\n",
    "    \"\"\"\n",
    "    Function to fetch stock information from a specified URL of the traderfox.de website\n",
    "    with an specified index / exchange\n",
    "\n",
    "    Inputs:\n",
    "        url (str): one of the available urls from traderfox.de\n",
    "        index_exchange (str): name of the index / exchnage from which\n",
    "            the stocks should be selected\n",
    "    \"\"\"\n",
    "    browser =  webdriver.Firefox()\n",
    "    time.sleep(2)\n",
    "    browser.get(url)\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(browser.page_source,\"html\")\n",
    "    time.sleep(5)\n",
    "    names = []\n",
    "    Row = soup.find('table', attrs = {'id' : 'insert-stocks'})\n",
    "    for name in Row.find_all('td', attrs = {'class' : 'name'}):\n",
    "            try:\n",
    "                names.append(name.text)\n",
    "            except:\n",
    "                names.append(\"\")\n",
    "\n",
    "    wkns = []\n",
    "    Row = soup.find('table', attrs = {'id' : 'insert-stocks'})\n",
    "    for wkn in Row.find_all('td', attrs = {'data-id' : 'wkn'}):\n",
    "            try:\n",
    "                wkns.append(wkn.text)\n",
    "            except:\n",
    "                wkns.append(\"\")\n",
    "\n",
    "    FRAME = pd.DataFrame.from_dict(\n",
    "            {'name': names,\n",
    "            'wkn': wkns,\n",
    "            'index' : index_exchange\n",
    "            })\n",
    "    time.sleep(1)\n",
    "    browser.quit()\n",
    "    return(FRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url_ls = [\"https://traderfox.de/aktien/alle-amex-aktien-bestandteile\"]#\"https://traderfox.de/aktien/alle-nasdaq-aktien-bestandteile\"]#]\n",
    "#  \"https://traderfox.de/aktien/alle-nyse-aktien-bestandteile\"]#,\n",
    "   # \"https://traderfox.de/aktien/deutschland-160-bestandteile\",\n",
    "   # \"https://traderfox.de/aktien/alle-nyse-aktien-bestandteile\",\n",
    "  #  \"https://traderfox.de/aktien/alle-nasdaq-aktien-bestandteile\",\n",
    "#]\n",
    "#https://traderfox.de/aktien/stoxx-europe-600-bestandteile\n",
    "country_names_ls = [\"AMEX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_frame = pd.DataFrame({})\n",
    "for i, url in enumerate(url_ls):\n",
    "    df = get_stock_data(url, country_names_ls[i])\n",
    "    ticker_frame = ticker_frame.append(df)\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker(initial_frame):\n",
    "    url_ticker = \"https://www.finanznachrichten.de/\"\n",
    "    url_yf_ticker = \"https://finance.yahoo.com/quote/FB?p=FB\"\n",
    "    browser =  webdriver.Firefox()\n",
    "    extension_path = r\"C:\\Users\\Jonathan\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\wl4weym2.default-1633941918487\\extensions\\jid1-MnnxcxisBPnSXQ@jetpack.xpi\"\n",
    "    browser.install_addon(extension_path, temporary=True)\n",
    "    time.sleep(2)\n",
    "    browser.get(\"about:support\")\n",
    "    time.sleep(5)\n",
    "    browser.get(url_ticker)\n",
    "    time.sleep(20)\n",
    "    #clear = WebDriverWait(browser, 5).until(EC.presence_of_element_located((By.XPATH, \"/html/body/div/div[2]/div[3]/div[2]/button\"))).click()\n",
    "    time.sleep(1)\n",
    "    ticker = []\n",
    "    de_ticker = []\n",
    "    industry = []\n",
    "    ISIN = []\n",
    "    for i, wkn in enumerate(initial_frame.wkn):\n",
    "        print(len(de_ticker), len(industry), len(ISIN))\n",
    "        # if wkn not valid\n",
    "        if wkn == \"-\":\n",
    "            de_ticker.append(\"not_found\")\n",
    "            industry.append(\"not_found\")\n",
    "            ISIN.append(\"not_found\")\n",
    "            continue\n",
    "        try:\n",
    "            clear = WebDriverWait(browser, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#fnk-suche-eingabe\"))).clear()\n",
    "            time.sleep(3)\n",
    "            search = WebDriverWait(browser, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#fnk-suche-eingabe\")))\n",
    "            time.sleep(1)\n",
    "            search.send_keys(str(wkn))\n",
    "            time.sleep(3)\n",
    "            browser.find_element_by_css_selector('#suchhilfeListe > tbody:nth-child(2) > tr:nth-child(3) > td:nth-child(2)').click()\n",
    "            time.sleep(random.randint(3,5))\n",
    "            de_ticker.append(str(WebDriverWait(browser, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#produkt-ticker\"))).text.replace(\"Ticker-Symbol: \", \"\").lstrip()))\n",
    "            #time.sleep(2)\n",
    "        except:\n",
    "            de_ticker.append(\"not_found\")\n",
    "            industry.append(\"not_found\")\n",
    "            ISIN.append(\"not_found\")\n",
    "            continue\n",
    "        try:\n",
    "            industry.append(str(WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".a > a:nth-child(2)\"))).text))\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            industry.append(\"not_found\")\n",
    "        try:\n",
    "            ISIN.append(str(WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#produkt-isin\"))).text.replace(\"ISIN: \", \"\").lstrip()))\n",
    "            time.sleep(1)\n",
    "            #time.sleep(random.randint(2,4))\n",
    "        except:\n",
    "            ISIN.append(\"not_found\")\n",
    "\n",
    "\n",
    "        if i % 4 == 0:\n",
    "            try:\n",
    "                browser.refresh()\n",
    "                time.sleep(4)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    #check if lengths are equal   \n",
    "    print(len(industry), len(ISIN), len(de_ticker))\n",
    "    try:\n",
    "        browser.get(url_yf_ticker)\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        browser.get(url_yf_ticker)\n",
    "        time.sleep(5)\n",
    "    try:\n",
    "        browser.find_element_by_css_selector(\"#scroll-down-btn\").click()\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        browser.find_element_by_css_selector(\"button.btn:nth-child(5)\").click()\n",
    "        time.sleep(4)\n",
    "    except:\n",
    "        pass\n",
    "    for _ISIN in ISIN:\n",
    "        if _ISIN == \"not_found\":\n",
    "            ticker.append(\"not_found\")\n",
    "            continue\n",
    "        time.sleep(2)\n",
    "        #check again for popup and scroll\n",
    "        try:\n",
    "            browser.find_element_by_css_selector(\"#scroll-down-btn\").click()\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            browser.find_element_by_css_selector(\"button.btn:nth-child(5)\").click()\n",
    "            time.sleep(4)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        clear = WebDriverWait(browser, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#yfin-usr-qry\"))).clear()\n",
    "        #time.sleep(1)\n",
    "        try:\n",
    "            search = WebDriverWait(browser, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#yfin-usr-qry\")))\n",
    "            #time.sleep(1)\n",
    "            search.send_keys(str(_ISIN))\n",
    "            time.sleep(2)\n",
    "            ticker.append(str(WebDriverWait(browser, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#result-quotes-0 > div.modules_quoteLeftCol__gkCSv.modules_Ell__77DLP.modules_IbBox__2pmLe > div.modules_quoteSymbol__hpPcM.modules_Ell__77DLP.modules_IbBox__2pmLe\"))).text))\n",
    "        except:\n",
    "            ticker.append(\"not_found\")\n",
    "            \n",
    "    print(len(ticker))      \n",
    "\n",
    "    frame = pd.DataFrame.from_dict(\n",
    "        {\"name\" : initial_frame.name,\n",
    "        \"wkn\" : initial_frame.wkn,\n",
    "        \"index\" : \"AMEX\",\n",
    "        'de_ticker' : de_ticker,\n",
    "        'ticker': ticker,\n",
    "        'industry': industry,\n",
    "        'ISIN': ISIN\n",
    "        })\n",
    "    time.sleep(1)\n",
    "    browser.quit()\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ticker_amex = get_ticker(ticker_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ticker_amex.to_csv(\"AMEX_Stocks.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-nasdaq 100\n",
    "\n",
    "-nyse\n",
    "\n",
    "-de\n",
    "\n",
    "-Eurostoxx600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = pd.read_csv(r\"C:\\Users\\Jonathan\\Documents\\GitHub\\finance_Versuchskaninchen\\fama_french\\scraper\\NASDAQStocks_p1.csv\")\n",
    "frame2 = pd.read_csv(r\"C:\\Users\\Jonathan\\Documents\\GitHub\\finance_Versuchskaninchen\\fama_french\\scraper\\NASDAQ_Stocks1197-2000.csv\")\n",
    "frame3 = pd.read_csv(r\"C:\\Users\\Jonathan\\Documents\\GitHub\\finance_Versuchskaninchen\\fama_french\\scraper\\NASDAQ_Stocks2000-rest.csv\")\n",
    "\n",
    "total_frame = pd.concat([frame1, frame2, frame3])\n",
    "total_frame.to_csv(\"NASDAQ_all_Stocks.csv\", encoding= 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ticker(path):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to update the stock information for the different exchanges / indicies\n",
    "    This function is usefull to:\n",
    "        - check if info is up to date\n",
    "        - after adding new stocks to the {exchange / index}Stocks.csv files\n",
    "        - to check that scraping was not corruped by bad internet connection or other issues\n",
    "        - to validate the scraping\n",
    "    \n",
    "    Inputs:\n",
    "        path (str): path of the file for which to perform the validation\n",
    "\n",
    "    \"\"\"\n",
    "    url_ticker = \"https://www.finanznachrichten.de/\"\n",
    "    url_yf_ticker = \"https://finance.yahoo.com/quote/FB?p=FB\"\n",
    "    # browser =  webdriver.Firefox(executable_path='/home/jonathan/Schreibtisch/geckodriver')\n",
    "    # extension_path = r\"/home/jonathan/.mozilla/firefox/5xapxbqn.default-release/extensions/jid1-MnnxcxisBPnSXQ@jetpack.xpi\"\n",
    "    # browser.install_addon(extension_path, temporary=True)\n",
    "\n",
    "\n",
    "    browser =  webdriver.Firefox()\n",
    "    extension_path = r\"C:\\Users\\Jonathan\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\wl4weym2.default-1633941918487\\extensions\\jid1-MnnxcxisBPnSXQ@jetpack.xpi\"\n",
    "    browser.install_addon(extension_path, temporary=True)\n",
    "    time.sleep(2)\n",
    "    #extension_path = r\"C:\\Users\\Jonathan\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\wl4weym2.default-1633941918487\\extensions\\firefox-webext@zenmate.com.xpi\"\n",
    "    #browser.install_addon(extension_path, temporary=True)\n",
    "    browser.maximize_window()\n",
    "    browser.get(\"about:support\")\n",
    "    time.sleep(5)\n",
    "    browser.get(url_ticker)\n",
    "    time.sleep(5)\n",
    "    file = pd.read_csv(path, encoding='utf-8')\n",
    "    try:\n",
    "        file.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        file.drop('Unnamed: 0.1', axis = 1, index = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for i, row in file.iterrows():\n",
    "        if (row.ISIN) == \"not_found\" and (row.wkn != \"-\"):\n",
    "            print(file.loc[i, \"name\"])\n",
    "            try:\n",
    "                time.sleep(1)\n",
    "                clear = WebDriverWait(browser, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#fnk-suche-eingabe\"))).clear()\n",
    "                time.sleep(random.randint(3,4))\n",
    "                search = WebDriverWait(browser, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#fnk-suche-eingabe\")))\n",
    "                time.sleep(random.randint(2,3))\n",
    "                search.send_keys(str(row.wkn))\n",
    "                time.sleep(3)\n",
    "                browser.find_element_by_css_selector('#suchhilfeListe > tbody:nth-child(2) > tr:nth-child(3) > td:nth-child(2)').click()\n",
    "                time.sleep(random.randint(2,3))\n",
    "                file.at[i, \"de_ticker\"] = (WebDriverWait(browser, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#produkt-ticker\"))).text.replace(\"Ticker-Symbol: \", \"\").lstrip())\n",
    "                #time.sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                if file.at[i, \"industry\"] == \"not_found\":\n",
    "                    file.at[i, \"industry\"] = (WebDriverWait(browser, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".a > a:nth-child(2)\"))).text)\n",
    "                    time.sleep(3)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                if file.at[i, \"ISIN\"] == \"not_found\":\n",
    "                    file.at[i, \"ISIN\"] = (WebDriverWait(browser, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#produkt-isin\"))).text.replace(\"ISIN: \", \"\").lstrip())\n",
    "                    time.sleep(random.randint(2,3))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    time.sleep(3)\n",
    "    browser.get(url_yf_ticker)\n",
    "    time.sleep(15)\n",
    "    try:\n",
    "        browser.find_element_by_css_selector(\"#scroll-down-btn\").click()\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        browser.find_element_by_css_selector(\"button.btn:nth-child(5)\").click()\n",
    "        time.sleep(4)\n",
    "    except:\n",
    "        pass\n",
    "    for i, row in file.iterrows():\n",
    "        if row.ISIN == \"not_found\":\n",
    "            continue\n",
    "        if file.at[i, \"ticker\"] == \"not_found\":\n",
    "            time.sleep(random.randint(1,2))\n",
    "            try:\n",
    "                browser.find_element_by_css_selector(\"#scroll-down-btn\").click()\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                browser.find_element_by_css_selector(\"button.btn:nth-child(5)\").click()\n",
    "                time.sleep(4)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            clear = WebDriverWait(browser, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#yfin-usr-qry\"))).clear()\n",
    "            time.sleep(random.randint(3,5))\n",
    "            try:\n",
    "                search = WebDriverWait(browser, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#yfin-usr-qry\")))\n",
    "                time.sleep(1)\n",
    "                search.send_keys(str(row.ISIN))\n",
    "                time.sleep(5)\n",
    "                file.at[i, \"ticker\"] = (WebDriverWait(browser, 25).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#result-quotes-0 > div.modules_quoteLeftCol__gkCSv.modules_Ell__77DLP.modules_IbBox__2pmLe > div.modules_quoteSymbol__hpPcM.modules_Ell__77DLP.modules_IbBox__2pmLe\"))).text)\n",
    "            except:\n",
    "                pass\n",
    "    #exchange = file.index[0]\n",
    "    file.to_csv(\"val_\" + path) \n",
    "\n",
    "    browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_ticker(\"val_AMEX_Stocks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Hard code fÃ¼r Hans martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "\n",
    "browser =  webdriver.Firefox()\n",
    "# extension_path = r\"C:\\Users\\Jonathan\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\wl4weym2.default-1633941918487\\extensions\\jid1-MnnxcxisBPnSXQ@jetpack.xpi\"\n",
    "# browser.install_addon(extension_path, temporary=True)\n",
    "# time.sleep(2)\n",
    "# extension_path = r\"C:\\Users\\Jonathan\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\wl4weym2.default-1633941918487\\extensions\\firefox-webext@zenmate.com.xpi\"\n",
    "# browser.install_addon(extension_path, temporary=True)\n",
    "# browser.get(\"about:support\")\n",
    "time.sleep(3)\n",
    "stocklist = [\"BC8.DE\", \"FEW.F\", \"2GB.DE\"]\n",
    "stock_df = pd.DataFrame(\n",
    "    {\n",
    "    })\n",
    "for stock in stocklist:\n",
    "    url = \"https://finance.yahoo.com/quote/\"+str(stock)+\"/key-statistics?p=\"+str(stock)\n",
    "\n",
    "    time.sleep(3)\n",
    "    browser.get(url)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        browser.find_element_by_css_selector(\"#scroll-down-btn\").click()\n",
    "        time.sleep(3)\n",
    "        browser.find_element_by_css_selector(\"button.btn:nth-child(5)\").click()\n",
    "    except:\n",
    "        pass\n",
    "    soup = BeautifulSoup(browser.page_source,\"html\")\n",
    "\n",
    "    data1 = browser.find_element_by_css_selector('table.W\\(100\\%\\):nth-child(1)')\n",
    "    first_table=[i for i in data1.text ]\n",
    "    first_table=''.join(first_table)\n",
    "    first_table = first_table.split('\\n')\n",
    "    second = False\n",
    "    while second != True:\n",
    "        try:\n",
    "            data2 = browser.find_element_by_css_selector('div.Fl\\(start\\):nth-child(3) > div:nth-child(2) > div:nth-child(2) > div:nth-child(1) > div:nth-child(1) > table:nth-child(2)')\n",
    "            second_table=[i for i in data2.text ]\n",
    "            second_table=''.join(second_table)\n",
    "            second_table = second_table.split('\\n')\n",
    "            second = True\n",
    "        except:\n",
    "            browser.refresh()\n",
    "    \n",
    "\n",
    "    data3 = browser.find_element_by_css_selector('div.Pos\\(r\\):nth-child(5) > div:nth-child(1) > div:nth-child(1) > table:nth-child(2)')\n",
    "    third_table=[i for i in data3.text ]\n",
    "    third_table=''.join(third_table)\n",
    "    third_table = third_table.split('\\n')\n",
    "\n",
    "    data4 = browser.find_element_by_css_selector('div.Fl\\(start\\):nth-child(3) > div:nth-child(2) > div:nth-child(3) > div:nth-child(1) > div:nth-child(1) > table:nth-child(2)')\n",
    "    fourth_table=[i for i in data4.text ]\n",
    "    fourth_table=''.join(fourth_table)\n",
    "    fourth_table = fourth_table.split('\\n')\n",
    "\n",
    "    data5 = browser.find_element_by_css_selector('div.Pstart\\(20px\\) > div:nth-child(2) > div:nth-child(1) > div:nth-child(1) > table:nth-child(2)')\n",
    "    fifth_table=[i for i in data5.text ]\n",
    "    fifth_table=''.join(fifth_table)\n",
    "    fifth_table = fifth_table.split('\\n')\n",
    "\n",
    "    \n",
    "    checklist = [\"Market\", \"Enterprise\", \"Trailing\", \"Forward\", \"PEG\", \"Price/Sales\", \"Price/Book\", \"Value/Revenue\", \"Value/EBITDA\",\n",
    "                \"Profit\", \"Operating\", \"Total\", \"Share\", \"Debt\", \"Ratio\", \"Book\"]\n",
    "    \n",
    "\n",
    "    final_table = first_table + second_table + third_table + fourth_table + fifth_table\n",
    "   \n",
    "    ###############\n",
    "    # Here check wheter all info correct\n",
    "    ###############\n",
    "    \n",
    "    # if not \"Market\" in new_Subject[0]:\n",
    "    #     new_Subject.insert(0,np.NaN)\n",
    "    \n",
    "    ITALIA2 = pd.DataFrame.from_dict(\n",
    "        \n",
    "        {str(stock): final_table\n",
    "    \n",
    "    }, orient=\"index\")\n",
    "    #print(ITALIA2)\n",
    "    stock_df = stock_df.append(ITALIA2)\n",
    "\n",
    "    \n",
    "print(stock_df)\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historic_price(query_url,json_path,csv_path):\n",
    "    \n",
    "    # while not check_internet():\n",
    "    #     print(\"Could not connect, trying again in 5 seconds...\")\n",
    "    #     time.sleep(5)\n",
    "    \n",
    "    stock_id=query_url.split(\"&period\")[0].split(\"symbol=\")[1]\n",
    "    \n",
    "    if os.path.exists(csv_path+stock_id+'.csv') and os.stat(csv_path+stock_id+'.csv').st_size != 0:\n",
    "        print(\"<<<  Historical data of \"+stock_id+\" already exists, Updating data...\")\n",
    "\n",
    "    try:\n",
    "        with urllib.request.urlopen(query_url) as url:\n",
    "            parsed = json.loads(url.read().decode())\n",
    "    except:\n",
    "        print(\"|||  Historical data of \"+stock_id+\" doesn't exist\")\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        if os.path.exists(json_path+stock_id+'.json'):\n",
    "            os.remove(json_path+stock_id+'.json')\n",
    "        with open(json_path+stock_id+'.json', 'w') as outfile:\n",
    "            json.dump(parsed, outfile, indent=4)\n",
    "\n",
    "        try:\n",
    "            Date=[]\n",
    "            for i in parsed['chart']['result'][0]['timestamp']:\n",
    "                Date.append(datetime.utcfromtimestamp(int(i)).strftime('%d-%m-%Y'))\n",
    "            Low=parsed['chart']['result'][0]['indicators']['quote'][0]['low']\n",
    "            Open=parsed['chart']['result'][0]['indicators']['quote'][0]['open']\n",
    "            Volume=parsed['chart']['result'][0]['indicators']['quote'][0]['volume']\n",
    "            High=parsed['chart']['result'][0]['indicators']['quote'][0]['high']\n",
    "            Close=parsed['chart']['result'][0]['indicators']['quote'][0]['close']\n",
    "            Adjusted_Close=parsed['chart']['result'][0]['indicators']['adjclose'][0]['adjclose']\n",
    "\n",
    "            df=pd.DataFrame(list(zip(Date,Low,Open,Volume,High,Close,Adjusted_Close)),columns =['Date','Low','Open','Volume','High','Close','Adjusted Close'])\n",
    "\n",
    "            if os.path.exists(csv_path+stock_id+'.csv'):\n",
    "                os.remove(csv_path+stock_id+'.csv')\n",
    "            df.to_csv(csv_path+stock_id+'.csv', sep=',', index=None)\n",
    "            print(\">>>  Historical data of \"+stock_id+\" saved\")\n",
    "            return\n",
    "        except:\n",
    "            print(\">>>  Historical data of \"+stock_id+\" exists but has no trading data\")\n",
    "json_path = os.getcwd()+os.sep+\"..\"+os.sep+\"historic_data\"+os.sep+\"json\"+os.sep\n",
    "csv_path = os.getcwd()+os.sep+\"..\"+os.sep+\"historic_data\"+os.sep+\"csv\"+os.sep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae004d2840ceac983a7d06d3a7778ed5b391f699d527375f584c8006e1f0288f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
